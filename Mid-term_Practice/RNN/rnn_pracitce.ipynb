{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rsurs\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\rsurs\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([148])) that is different to the input size (torch.Size([148, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 | Train loss: 0.01829891072379218\n",
      "Epoch : 1 | Train loss: 0.015761627091301814\n",
      "Epoch : 2 | Train loss: 0.010942712426185608\n",
      "Epoch : 3 | Train loss: 0.009428432418240441\n",
      "Epoch : 4 | Train loss: 0.007589219344986809\n",
      "Epoch : 5 | Train loss: 0.006158503807253308\n",
      "Epoch : 6 | Train loss: 0.005209334194660187\n",
      "Epoch : 7 | Train loss: 0.004767655498451657\n",
      "Epoch : 8 | Train loss: 0.004312836461597019\n",
      "Epoch : 9 | Train loss: 0.0034642256796360016\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = pd.read_csv('daily_csv.csv')\n",
    "df.dropna(inplace=True)\n",
    "y = df['Price'].values.reshape(-1,1)\n",
    "sc = MinMaxScaler()\n",
    "y = sc.fit_transform(y).flatten()\n",
    "x = np.arange(1,len(y),1)\n",
    "seq_len = 10\n",
    "X,Y = [],[]\n",
    "for i in range(0,5000):\n",
    "    ls = []\n",
    "    for j in range(i, i + seq_len):\n",
    "        ls.append(y[j])\n",
    "    X.append(ls)\n",
    "    Y.append(y[j + 1])\n",
    "X,Y = np.array(X),np.array(Y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.1, random_state = 42, shuffle = False, stratify = None)\n",
    "\n",
    "class custom_dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.tensor(x, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.len = x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "data = custom_dataset(x_train, y_train)\n",
    "train_loader = DataLoader(data, shuffle = True, batch_size = 256)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size = 1, hidden_size = 5, num_layers = 1, batch_first = True)\n",
    "        self.fc1 = nn.Linear(in_features = 5, out_features = 1)\n",
    "\n",
    "    def forward(self,x ):\n",
    "        output, _status = self.rnn(x)\n",
    "        output = output[:,-1,:]\n",
    "        output = self.fc1(torch.relu(output))\n",
    "        return output\n",
    "\n",
    "model = RNN().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "losses=[]\n",
    "for epoch in range(10):\n",
    "    for i,(X,y) in enumerate(train_loader):\n",
    "        X,y = X.to(device),y.to(device)\n",
    "        X = X.view(-1,seq_len,1)\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred,y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch : {epoch} | Train loss: {loss.item()/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 1 | Loss: 1.8706881320400603\n",
      "Epochs 2 | Loss: 1.8539345865093855\n",
      "Epochs 3 | Loss: 1.8542086849364254\n",
      "Epochs 4 | Loss: 1.8545676530182225\n",
      "Epochs 5 | Loss: 1.8543791539446277\n",
      "predicted nationality of alex is Russian\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import glob \n",
    "import torch \n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from pathlib import Path \n",
    "\n",
    "all_names,labels = [],[]\n",
    "max_len_name = 0\n",
    "base_dir = Path('data/dataset/names')\n",
    "# print(base_dir)\n",
    "file_paths = list(base_dir.glob('*'))\n",
    "# print(file_paths)\n",
    "categories = [file_path.stem for file_path in file_paths]\n",
    "# print(categories)\n",
    "num_classes = len(categories)\n",
    "# print(num_classes)\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        names = f.read().split('\\n')\n",
    "        for name in names:\n",
    "            if len(name) > max_len_name:\n",
    "                max_len_name = len(name)\n",
    "# print(max_len_name)\n",
    "\n",
    "for i,file_path in enumerate(file_paths):\n",
    "    nationality_index = categories.index(file_path.stem)\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        names = f.read().split('\\n')\n",
    "        for name in names:\n",
    "            num_list = [ord(k) for k in name] + [0]*(max_len_name - len(name))\n",
    "            all_names.append(num_list)\n",
    "            labels.append(nationality_index)\n",
    "            \n",
    "class Data(Dataset):\n",
    "    def __init__(self,X,y):\n",
    "        super().__init__()\n",
    "        self.X = torch.tensor(X,dtype=torch.float32)\n",
    "        self.y = torch.tensor(y,dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index],self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "class RNN_Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size,hidden_size=hidden_size,num_layers=1,batch_first=True)\n",
    "        self.fc1 = nn.Linear(in_features=hidden_size,out_features=num_classes)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        output, _ = self.rnn(X)\n",
    "        output = output[:,-1,:]\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "data = Data(all_names,labels)\n",
    "data_loader = DataLoader(data, batch_size=32, shuffle=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = RNN_Model(input_size=1,hidden_size=12,num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.005)\n",
    "\n",
    "epochs = 5\n",
    "train_loss = 0\n",
    "for epoch in range(epochs):\n",
    "    for batch,(X,y) in enumerate(data_loader):\n",
    "        X,y = X.to(device),y.to(device)\n",
    "        X = X.view(-1,max_len_name,1)\n",
    "        y_pred = model(X)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(y_pred,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(data_loader)\n",
    "    print(f\"Epochs {epoch + 1} | Loss: {train_loss}\")\n",
    "\n",
    "def predict(model,name,max_len_name,categories,device):\n",
    "    name_ascii = [ord(char) for char in name] + [0] * (max_len_name - len(name))\n",
    "    name_tensor = torch.tensor([name_ascii],dtype=torch.float32).to(device)\n",
    "    name_tensor = name_tensor.view(-1,max_len_name,1)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        output = model(name_tensor)\n",
    "        _,predicted_index = torch.max(output,1)\n",
    "        nationality = categories[predicted_index]\n",
    "    return nationality\n",
    "\n",
    "sample = 'alex'\n",
    "sample_out = predict(model,sample,max_len_name,categories,device)\n",
    "print(f\"predicted nationality of {sample} is {sample_out}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\dataset\\names\n",
      "[WindowsPath('data/dataset/names/Arabic.txt'), WindowsPath('data/dataset/names/Chinese.txt'), WindowsPath('data/dataset/names/Czech.txt'), WindowsPath('data/dataset/names/Dutch.txt'), WindowsPath('data/dataset/names/English.txt'), WindowsPath('data/dataset/names/French.txt'), WindowsPath('data/dataset/names/German.txt'), WindowsPath('data/dataset/names/Greek.txt'), WindowsPath('data/dataset/names/Irish.txt'), WindowsPath('data/dataset/names/Italian.txt'), WindowsPath('data/dataset/names/Japanese.txt'), WindowsPath('data/dataset/names/Korean.txt'), WindowsPath('data/dataset/names/Polish.txt'), WindowsPath('data/dataset/names/Portuguese.txt'), WindowsPath('data/dataset/names/Russian.txt'), WindowsPath('data/dataset/names/Scottish.txt'), WindowsPath('data/dataset/names/Spanish.txt'), WindowsPath('data/dataset/names/Vietnamese.txt')]\n",
      "Epoch 1 | Loss: 0.0007172762745558613\n",
      "Epoch 2 | Loss: 0.000622891075266391\n",
      "Epoch 3 | Loss: 0.0006781061332621962\n",
      "Epoch 4 | Loss: 0.0006088418364844194\n",
      "Epoch 5 | Loss: 0.0006864633851488606\n",
      "Predicted character: v\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import glob \n",
    "import torch \n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from pathlib import Path \n",
    "base_dir = Path('data/dataset/names')\n",
    "print(base_dir)\n",
    "file_paths = list(base_dir.glob('*'))\n",
    "print(file_paths)\n",
    "all_chars = set()\n",
    "for file_path in file_paths:\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        names = f.read().split('\\n')\n",
    "        for name in names:\n",
    "            all_chars.update(name)\n",
    "\n",
    "all_chars = sorted(list(all_chars))\n",
    "num_chars = len(all_chars)\n",
    "char_to_idx = {ch:i for i,ch in enumerate(all_chars)}\n",
    "index_to_char = {i:ch for i,ch in enumerate(all_chars)}\n",
    "\n",
    "max_sequence_length = 10\n",
    "sequences,next_chars = [],[]\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        names = f.read().split('\\n')\n",
    "        for name in names:\n",
    "            for i in range(len(name) - 1):\n",
    "                start_index = max(0,i + 1 - max_sequence_length)\n",
    "                end_index = i + 1\n",
    "                sequence = [char_to_idx[ch] for ch in name[start_index:end_index]]\n",
    "                sequence += [0]*(max_sequence_length - len(sequence))\n",
    "                sequences.append(sequence)\n",
    "                next_chars.append(char_to_idx[name[i + 1]])\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self,sequences,next_chars):\n",
    "        super().__init__()\n",
    "        self.sequences = torch.tensor(sequences,dtype=torch.long)\n",
    "        self.next_chars = torch.tensor(next_chars,dtype=torch.long)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index],self.next_chars[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "class CharModel(nn.Module):\n",
    "    def __init__(self, num_chars, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_chars,embedding_dim=hidden_size)\n",
    "        self.rnn = nn.RNN(input_size=hidden_size,hidden_size=hidden_size,batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=hidden_size,out_features=num_chars)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x)\n",
    "        output,_ = self.rnn(x)\n",
    "        output = output[:,-1,:]\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "dataset = Data(sequences,next_chars)\n",
    "data_loader = DataLoader(dataset,batch_size=32,shuffle=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CharModel(num_chars=num_chars,hidden_size=100).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.005)\n",
    "epochs = 5\n",
    "train_loss = 0 \n",
    "for epoch in range(epochs):\n",
    "    for batch,(seq,next_char) in enumerate(data_loader):\n",
    "        seq,next_char = seq.to(device),next_char.to(device)\n",
    "        output = model(seq)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output,next_char)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "    train_loss /= len(data_loader)\n",
    "    print(f\"Epoch {epoch + 1} | Loss: {train_loss}\")\n",
    "\n",
    "def predict(model, char_to_idx, idx_to_char, sequence, hidden_size=100, device='cpu'):\n",
    "    sequence = sequence[-max_sequence_length:]\n",
    "    sequence_idx = [char_to_idx.get(ch, 0) for ch in sequence]\n",
    "    sequence_idx += [0] * (max_sequence_length - len(sequence_idx))\n",
    "    input_tensor = torch.tensor([sequence_idx], dtype=torch.long).to(device)  # Corrected line\n",
    "    model.eval()\n",
    "    with torch.no_grad():  # Updated to use torch.no_grad() for consistency\n",
    "        input_tensor = input_tensor.to(device)  # Ensure tensor is on the correct device\n",
    "        pred = model(input_tensor)\n",
    "        pred_idx = pred.argmax(dim=1).item()\n",
    "    next_char = idx_to_char[pred_idx]\n",
    "    return next_char\n",
    "\n",
    "# Example usage, ensuring the device is correctly used\n",
    "pred_char = predict(model, char_to_idx, index_to_char, 'hello', device=device)\n",
    "print(f\"Predicted character: {pred_char}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\dataset\\names\n",
      "[WindowsPath('data/dataset/names/Arabic.txt'), WindowsPath('data/dataset/names/Chinese.txt'), WindowsPath('data/dataset/names/Czech.txt'), WindowsPath('data/dataset/names/Dutch.txt'), WindowsPath('data/dataset/names/English.txt'), WindowsPath('data/dataset/names/French.txt'), WindowsPath('data/dataset/names/German.txt'), WindowsPath('data/dataset/names/Greek.txt'), WindowsPath('data/dataset/names/Irish.txt'), WindowsPath('data/dataset/names/Italian.txt'), WindowsPath('data/dataset/names/Japanese.txt'), WindowsPath('data/dataset/names/Korean.txt'), WindowsPath('data/dataset/names/Polish.txt'), WindowsPath('data/dataset/names/Portuguese.txt'), WindowsPath('data/dataset/names/Russian.txt'), WindowsPath('data/dataset/names/Scottish.txt'), WindowsPath('data/dataset/names/Spanish.txt'), WindowsPath('data/dataset/names/Vietnamese.txt')]\n",
      "['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German', 'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish', 'Portuguese', 'Russian', 'Scottish', 'Spanish', 'Vietnamese']\n",
      "18\n",
      "20\n",
      "20092 20092\n",
      "Epoch: 1 | Train loss:1.8726075050557496\n",
      "Epoch: 2 | Train loss:1.8549347051575251\n",
      "Epoch: 3 | Train loss:1.854738986816217\n",
      "Epoch: 4 | Train loss:1.8532802444170082\n",
      "Epoch: 5 | Train loss:1.8540580651343548\n",
      "The predicted nationality for Alexander is Russian.\n"
     ]
    }
   ],
   "source": [
    "import glob \n",
    "import os \n",
    "import torch \n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path('data/dataset/names')\n",
    "print(base_dir)\n",
    "file_paths = list(base_dir.glob('*'))\n",
    "print(file_paths)\n",
    "all_names,labels = [],[]\n",
    "max_len = 0\n",
    "categories = [file_path.stem for file_path in file_paths]\n",
    "print(categories)\n",
    "num_classes = len(categories)\n",
    "print(num_classes)\n",
    "for file_path in file_paths:\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        names = f.read().split('\\n')\n",
    "        for name in names:\n",
    "            if len(name) > max_len:\n",
    "                max_len = len(name)\n",
    "print(max_len)\n",
    "\n",
    "for i,file_path in enumerate(file_paths):\n",
    "    category_index = categories.index(file_path.stem)\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        names = f.read().split('\\n')\n",
    "        for name in names:\n",
    "            num_list = [ord(char) for char in name] + [0]*(max_len - len(name))\n",
    "            all_names.append(num_list)\n",
    "            labels.append(category_index)\n",
    "print(len(all_names),len(labels))\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self,X,y):\n",
    "        super().__init__()\n",
    "        self.X = torch.tensor(X,dtype=torch.float32)\n",
    "        self.y = torch.tensor(y,dtype=torch.long)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index],self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "class RNN_Char(nn.Module):\n",
    "    def __init__(self, input_size,hidden_size,output_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size,hidden_size=hidden_size,num_layers=1,batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=hidden_size,out_features=output_size)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        output,_ = self.rnn(X)\n",
    "        output = output[:,-1,:]\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "dataset = Data(all_names,labels)\n",
    "data_loader = DataLoader(dataset=dataset,batch_size=32,shuffle=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = RNN_Char(input_size=1,hidden_size=10,output_size=num_classes).to(device)\n",
    "model \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.005)\n",
    "epochs = 5\n",
    "train_loss = 0\n",
    "for epoch in range(epochs):\n",
    "    for batch,(X,y) in enumerate(data_loader):\n",
    "        X,y = X.to(device),y.to(device)\n",
    "        X = X.view(-1,max_len,1)\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred,y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(data_loader)\n",
    "    print(f\"Epoch: {epoch + 1} | Train loss:{train_loss}\")\n",
    "    \n",
    "def predict(model,categories,name,max_len,device):\n",
    "    name_ascii = [ord(char) for char in name] + [0] * (max_len - len(name))\n",
    "    name_tensor = torch.tensor([name_ascii],dtype=torch.float32)\n",
    "    name_tensor = name_tensor.view(-1,max_len,1).to(device)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(name_tensor)\n",
    "        pred_idx = y_pred.argmax(dim=1)\n",
    "    nationality_index = categories[pred_idx.item()]\n",
    "    return nationality_index        \n",
    "\n",
    "sample_input = 'Alexander'\n",
    "sample_out = predict(model, categories, sample_input, max_len, device)\n",
    "print(f\"The predicted nationality for {sample_input} is {sample_out}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 2.9051\n",
      "Epoch 2/5, Loss: 2.6333\n",
      "Epoch 3/5, Loss: 2.6053\n",
      "Epoch 4/5, Loss: 2.6094\n",
      "Epoch 5/5, Loss: 2.6176\n"
     ]
    }
   ],
   "source": [
    "import glob \n",
    "import os \n",
    "import torch \n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "all_chars = set()\n",
    "next_chars,sequences = [],[]\n",
    "max_seq_len = 10\n",
    "base_dir = Path('data/dataset/names')\n",
    "file_paths = list(base_dir.glob('*'))\n",
    "for file_path in file_paths:\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        names = f.read().split('\\n')\n",
    "        for name in names:\n",
    "            all_chars.update(name)\n",
    "all_chars = sorted(list(all_chars))\n",
    "num_chars = len(all_chars)\n",
    "char_to_idx = {ch:i for i,ch in enumerate(all_chars)}\n",
    "index_to_char = {i:ch for i,ch in enumerate(all_chars)}\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        names = f.read().split('\\n')\n",
    "        for name in names:\n",
    "            for i in range(len(name) - 1):\n",
    "                start_index = max(0,i+1-max_seq_len)\n",
    "                end_index = i + 1\n",
    "                sequence = [char_to_idx[ch] for ch in name[start_index:end_index]]\n",
    "                sequence += [0]*(max_seq_len - len(sequence))\n",
    "                sequences.append(sequence)\n",
    "                next_chars.append(char_to_idx[name[i + 1]])\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self,sequences,next_chars):\n",
    "        self.sequences = torch.tensor(sequences,dtype=torch.long)\n",
    "        self.next_chars = torch.tensor(next_chars,dtype=torch.long)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index],self.next_chars[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "dataset = Data(sequences,next_chars)\n",
    "data_loader = DataLoader(dataset=dataset,batch_size=32,shuffle=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class RNN_Model(nn.Module):\n",
    "    def __init__(self, num_chars,hidden_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=num_chars,embedding_dim=hidden_size)\n",
    "        self.rnn = nn.RNN(input_size=hidden_size,hidden_size=hidden_size,num_layers=1,batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=hidden_size,out_features=num_chars)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.embeddings(x)\n",
    "        output,_ = self.rnn(x)\n",
    "        output = output[:,-1,:]\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = RNN_Model(num_chars,hidden_size=100).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for seq, next_char in data_loader:\n",
    "        seq, next_char = seq.to(device), next_char.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(seq)\n",
    "        loss = criterion(output, next_char)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
